{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 19:30:23.426149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 19:30:25.165506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py:62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m stateful_random_ops\n\u001b[1;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m while_v2\n\u001b[0;32m---> 62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distributions.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# pylint: disable=wildcard-import,unused-import,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m deprecation\u001b[39m.\u001b[39msilence():\n\u001b[0;32m---> 21\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbernoulli\u001b[39;00m \u001b[39mimport\u001b[39;00m Bernoulli\n\u001b[1;32m     22\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbeta\u001b[39;00m \u001b[39mimport\u001b[39;00m Beta\n\u001b[1;32m     23\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcategorical\u001b[39;00m \u001b[39mimport\u001b[39;00m Categorical\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:32\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecation\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n\u001b[1;32m     31\u001b[0m \u001b[39m@tf_export\u001b[39;49m(v1\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mdistributions.Bernoulli\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m---> 32\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mBernoulli\u001b[39;49;00m(distribution\u001b[39m.\u001b[39;49mDistribution):\n\u001b[1;32m     33\u001b[0m \u001b[39m  \u001b[39;49m\u001b[39m\"\"\"Bernoulli distribution.\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[39m  The Bernoulli distribution with `probs` parameter, i.e., the probability of a\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m  `1` outcome (vs a `0` outcome).\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m  \"\"\"\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m   \u001b[39m@deprecation\u001b[39;49m\u001b[39m.\u001b[39;49mdeprecated(\n\u001b[1;32m     40\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39m2019-01-01\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     41\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mThe TensorFlow Distributions library has moved to \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                allow_nan_stats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     53\u001b[0m                name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBernoulli\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py:349\u001b[0m, in \u001b[0;36mapi_export.__call__\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    347\u001b[0m _, undecorated_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(func)\n\u001b[1;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_attr(undecorated_func, api_names_attr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_names)\n\u001b[0;32m--> 349\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_attr(undecorated_func, api_names_attr_v1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_names_v1)\n\u001b[1;32m    351\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_names:\n\u001b[1;32m    352\u001b[0m   _NAME_TO_SYMBOL_MAPPING[name] \u001b[39m=\u001b[39m func\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "DATA_PATH           = 'data.json'\n",
    "SAVED_MODEL_PATH    = 'model.h5'\n",
    "LEARNING_RATE       = 0.0001\n",
    "BATCH_SIZE          = 48\n",
    "NUMBER_OF_EPOCHS    = 40\n",
    "NUMBER_OF_KEYWORDS  = 30\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    # load from json\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    X = np.array(data[\"MFCCs\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_split(data_path):\n",
    "    # load dataset\n",
    "    X, y = load_dataset(data_path)\n",
    "\n",
    "    # create train/validation/test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    # convert inputs from 2D-arrays to 3D-arrays\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val = X_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, lr, number_of_classes, error='sparse_categorical_crossentropy'):\n",
    "    # Build network\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # conv layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    # layer norm\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "    # conv layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    # layer norm\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "    # conv layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    # layer norm\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    # flatten output\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # dense layer 1\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    # dropout \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    # dense layer 2 (softmax)\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=error,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # show model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "def train():\n",
    "\n",
    "    # load train/validation/test data splits\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_data_split(DATA_PATH)\n",
    "\n",
    "    # Build the CNN model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]) # (# segments, # coefficients=13, # channels=1)\n",
    "    model = build_model(input_shape=input_shape, \n",
    "                        lr=LEARNING_RATE,\n",
    "                        number_of_classes=NUMBER_OF_KEYWORDS)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=NUMBER_OF_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_validation, y_validation))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_error, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test error: {test_error}, test accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save(SAVED_MODEL_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
